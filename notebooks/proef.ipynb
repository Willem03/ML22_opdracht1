{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 08:17:12.262489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-24 08:17:13.286816: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-24 08:17:13.286943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-24 08:17:13.286957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, \"../..\")\n",
    "\n",
    "from typing import Callable, Protocol, Dict, Optional, Iterator, List, Tuple\n",
    "\n",
    "import gin\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# ------------------------------------------------------------------------------- #\n",
    "## Alle imports benodigd voor de functie train_loop uit train_model.py \n",
    "\n",
    "import tensorflow as tf  # noqa: F401\n",
    "\n",
    "# needed to make summarywriter load without error\n",
    "from loguru import logger\n",
    "from numpy import Inf\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benodigde functie uitmake_dataset.py\n",
    "\n",
    "@gin.configurable\n",
    "def get_MNIST(  # noqa: N802\n",
    "    data_dir: Path, batch_size: int\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benodigde funties uit metrics.py\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class Metric:\n",
    "    def __repr__(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Accuracy\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        return (yhat.argmax(dim=1) == y).sum() / len(yhat)\n",
    "\n",
    "class GenericModel(Protocol):\n",
    "    train: Callable\n",
    "    eval: Callable\n",
    "    parameters: Callable\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "def count_parameters(model: GenericModel) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "## uit data_tools.py\n",
    "def dir_add_timestamp(log_dir: Optional[Path] = None) -> Path:\n",
    "    if log_dir is None:\n",
    "        log_dir = Path(\".\")\n",
    "    log_dir = Path(log_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    log_dir = log_dir / timestamp\n",
    "    logger.info(f\"Logging to {log_dir}\")\n",
    "    if not log_dir.exists():\n",
    "        log_dir.mkdir(parents=True)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren functie train_loop\n",
    "\n",
    "@gin.configurable\n",
    "def trainloop(\n",
    "    epochs: int,\n",
    "    model: GenericModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    learning_rate: float,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    train_dataloader: Iterator,\n",
    "    test_dataloader: Iterator,\n",
    "    log_dir: Path,\n",
    "    train_steps: int,\n",
    "    eval_steps: int,\n",
    "    patience: int = 10,\n",
    "    factor: float = 0.9,\n",
    "    tunewriter: bool = False,\n",
    ") -> GenericModel:\n",
    "    \n",
    "    optimizer_: torch.optim.Optimizer = optimizer(\n",
    "        model.parameters(), lr=learning_rate\n",
    "    )  # type: ignore\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_,\n",
    "        factor=factor,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    if not tunewriter:\n",
    "        log_dir = dir_add_timestamp(log_dir)\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        #write_gin(log_dir, gin.config_str())\n",
    "\n",
    "        images, _ = next(iter(train_dataloader))\n",
    "        if len(images.shape) == 4:\n",
    "            grid = make_grid(images)\n",
    "            writer.add_image(\"images\", grid)\n",
    "        writer.add_graph(model, images)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = trainbatches(\n",
    "            model, train_dataloader, loss_fn, optimizer_, train_steps\n",
    "        )\n",
    "\n",
    "        metric_dict, test_loss = evalbatches(\n",
    "            model, test_dataloader, loss_fn, metrics, eval_steps\n",
    "        )\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        if tunewriter:\n",
    "            tune.report(\n",
    "                iterations=epoch,\n",
    "                train_loss=train_loss,\n",
    "                test_loss=test_loss,\n",
    "                **metric_dict,\n",
    "            )\n",
    "        else:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "            for m in metric_dict:\n",
    "                writer.add_scalar(f\"metric/{m}\", metric_dict[m], epoch)\n",
    "            lr = [group[\"lr\"] for group in optimizer_.param_groups][0]\n",
    "            writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "            metric_scores = [f\"{v:.4f}\" for v in metric_dict.values()]\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} train {train_loss:.4f} test {test_loss:.4f} metric {metric_scores}\"  # noqa E501\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit train_model.py\n",
    "\n",
    "def trainbatches(\n",
    "    model: GenericModel,\n",
    "    traindatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_steps: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss: float = 0.0\n",
    "    for _ in tqdm(range(train_steps)):\n",
    "        x, y = next(iter(traindatastreamer))\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.detach().numpy()\n",
    "    train_loss /= train_steps\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "## uit train_model.py\n",
    "def evalbatches(\n",
    "    model: GenericModel,\n",
    "    testdatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    eval_steps: int,\n",
    ") -> Tuple[Dict[str, float], float]:\n",
    "    model.eval()\n",
    "    test_loss: float = 0.0\n",
    "    metric_dict: Dict[str, float] = {}\n",
    "    for _ in range(eval_steps):\n",
    "        x, y = next(iter(testdatastreamer))\n",
    "        yhat = model(x)\n",
    "        test_loss += loss_fn(yhat, y).detach().numpy()\n",
    "        for m in metrics:\n",
    "            metric_dict[str(m)] = (\n",
    "                metric_dict.get(str(m), 0.0) + m(y, yhat).detach().numpy()\n",
    "            )\n",
    "\n",
    "    test_loss /= eval_steps\n",
    "    for key in metric_dict:\n",
    "        metric_dict[str(key)] = metric_dict[str(key)] / eval_steps\n",
    "    return metric_dict, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 1: CNN_een met een convolutional layer & drie linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_1_3(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 3136),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3136, 1568),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1568, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2: CNN_twee met twee convolutional layers & twee linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_2(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2.16.32: CNN_twee met twee convolutional layers & twee linear layers & 16-32 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_2_16_32(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2.32.64: CNN_twee met twee convolutional layers & twee linear layers & 32-64 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_2_32_64(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2.64.128: CNN_twee met twee convolutional layers & twee linear layers & 64-128 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_2_64_128(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4608, 2304),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2304, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2.128.256: CNN_twee met twee convolutional layers & twee linear layers & 128-256 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_2_128_256(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(9216, 4608),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4608, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.3: CNN_twee met twee convolutional layers en drie linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_3(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.3.16.32: CNN_twee met twee convolutional layers en drie linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_3_16_32(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.3.32.64: CNN_twee met twee convolutional layers en drie linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_3_32_64(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.3.64.128: CNN_twee met twee convolutional layers en drie linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_3_64_128(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4608, 2304),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.4: CNN_twee met twee convolutional layers & vier linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_2_4(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.2: CNN_drie met twee convolutional layers & twee linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_2(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.2.16.32: CNN_drie met twee convolutional layers & twee linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_2_16_32(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.2.16.32: CNN_drie met twee convolutional layers & twee linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_2_32_64(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.3: CNN_drie met drie convolutional layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_3(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.3.16.32: CNN_drie met drie convolutional layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_3_16_32(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(144, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.3.32.64: CNN_drie met drie convolutional layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_3_32_64(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4: CNN_drie met drie convolutional layers & vier linear layers\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_4(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4.16.32: CNN_twee met twee convolutional layers & twee linear layers & 64-128 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_4_16_32(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4.64.128: CNN_twee met twee convolutional layers & twee linear layers & 64-128 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_4_32_64(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4.64.128: CNN_twee met twee convolutional layers & twee linear layers & 64-128 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_4_64_128(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4608, 2304),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4.128.256: CNN_twee met twee convolutional layers & twee linear layers & 64-128 filters\n",
    "\n",
    "@gin.configurable\n",
    "class CNN_3_4_128_256(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(9216, 4608),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4608, 2304),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1152, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='model.gin', imports=['gin.torch.external_configurables'], includes=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(summary(model))\n",
    "#print(gin.config_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir =  Path(\"/home/admindme/code/ML22_opdracht1/data/raw/FashionMNIST\")\n",
    "train_dataloader, test_dataloader = get_MNIST(datadir, 64)\n",
    "len(train_dataloader), len(test_dataloader)\n",
    "accuracy = Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 15:51:35.289 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-1551\n",
      "100%|██████████| 938/938 [03:52<00:00,  4.04it/s]\n",
      "2022-12-23 15:55:32.359 | INFO     | __main__:trainloop:68 - Epoch 0 train 43.8874 test 2.3037 metric ['0.1069']\n",
      "100%|██████████| 938/938 [05:02<00:00,  3.10it/s]\n",
      "2022-12-23 16:00:39.257 | INFO     | __main__:trainloop:68 - Epoch 1 train 2.3066 test 2.3079 metric ['0.0950']\n",
      "100%|██████████| 938/938 [05:02<00:00,  3.10it/s]\n",
      "2022-12-23 16:05:46.406 | INFO     | __main__:trainloop:68 - Epoch 2 train 2.3071 test 2.3080 metric ['0.0979']\n",
      "100%|██████████| 3/3 [14:10<00:00, 283.63s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 1 \n",
    "#model = CNN_1_3().to(device)\n",
    "\n",
    "#model = trainloop(\n",
    " #               model=model,\n",
    "  #              metrics=[accuracy],\n",
    "   #             train_dataloader=train_dataloader,\n",
    "    #            test_dataloader=test_dataloader,\n",
    "     #           train_steps=len(train_dataloader),\n",
    "      #          eval_steps=150,\n",
    "       #         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 19:25:52.473 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-1925\n",
      "100%|██████████| 938/938 [00:34<00:00, 26.83it/s]\n",
      "2022-12-23 19:26:30.446 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.5847 test 0.4695 metric ['0.8375']\n",
      "100%|██████████| 938/938 [00:36<00:00, 25.44it/s]\n",
      "2022-12-23 19:27:09.411 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4138 test 0.4384 metric ['0.8357']\n",
      "100%|██████████| 938/938 [00:36<00:00, 25.44it/s]\n",
      "2022-12-23 19:27:48.474 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3785 test 0.3789 metric ['0.8573']\n",
      "100%|██████████| 938/938 [00:36<00:00, 25.61it/s]\n",
      "2022-12-23 19:28:27.240 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3652 test 0.3838 metric ['0.8612']\n",
      "100%|██████████| 938/938 [00:36<00:00, 25.95it/s]\n",
      "2022-12-23 19:29:05.451 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3380 test 0.3648 metric ['0.8704']\n",
      "100%|██████████| 5/5 [03:12<00:00, 38.43s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 1 \n",
    "model = CNN_2_3().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:20:09.950 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2020\n",
      "100%|██████████| 938/938 [00:29<00:00, 32.08it/s]\n",
      "2022-12-23 20:20:41.517 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.5654 test 0.4180 metric ['0.8495']\n",
      "100%|██████████| 938/938 [00:29<00:00, 31.50it/s]\n",
      "2022-12-23 20:21:13.407 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.3716 test 0.4064 metric ['0.8465']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.92it/s]\n",
      "2022-12-23 20:21:45.898 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3417 test 0.3641 metric ['0.8685']\n",
      "100%|██████████| 938/938 [00:24<00:00, 38.41it/s]\n",
      "2022-12-23 20:22:12.420 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3225 test 0.3725 metric ['0.8714']\n",
      "100%|██████████| 938/938 [00:24<00:00, 37.94it/s]\n",
      "2022-12-23 20:22:39.271 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3181 test 0.3542 metric ['0.8769']\n",
      "100%|██████████| 5/5 [02:29<00:00, 29.81s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 1 \n",
    "model = CNN_3_3().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 19:35:27.675 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-1935\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.13it/s]\n",
      "2022-12-23 19:36:02.293 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.4888 test 0.4435 metric ['0.8359']\n",
      "100%|██████████| 938/938 [00:33<00:00, 27.84it/s]\n",
      "2022-12-23 19:36:38.266 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.3655 test 0.3912 metric ['0.8631']\n",
      "100%|██████████| 938/938 [00:33<00:00, 28.24it/s]\n",
      "2022-12-23 19:37:13.771 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3425 test 0.3747 metric ['0.8682']\n",
      "100%|██████████| 938/938 [00:33<00:00, 28.03it/s]\n",
      "2022-12-23 19:37:49.493 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3254 test 0.3899 metric ['0.8622']\n",
      "100%|██████████| 938/938 [00:33<00:00, 28.04it/s]\n",
      "2022-12-23 19:38:25.146 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3099 test 0.3787 metric ['0.8705']\n",
      "100%|██████████| 5/5 [02:57<00:00, 35.46s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 2 \n",
    "model = CNN_2_2().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 19:38:41.179 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-1938\n",
      "100%|██████████| 938/938 [00:35<00:00, 26.65it/s]\n",
      "2022-12-23 19:39:18.865 | INFO     | __main__:trainloop:68 - Epoch 0 train 2.3051 test 2.3036 metric ['0.1013']\n",
      "100%|██████████| 938/938 [00:38<00:00, 24.64it/s]\n",
      "2022-12-23 19:39:59.203 | INFO     | __main__:trainloop:68 - Epoch 1 train 2.3033 test 2.3032 metric ['0.0977']\n",
      "100%|██████████| 938/938 [00:37<00:00, 24.78it/s]\n",
      "2022-12-23 19:40:39.380 | INFO     | __main__:trainloop:68 - Epoch 2 train 2.3035 test 2.3041 metric ['0.1016']\n",
      "100%|██████████| 938/938 [00:37<00:00, 25.14it/s]\n",
      "2022-12-23 19:41:19.042 | INFO     | __main__:trainloop:68 - Epoch 3 train 2.3035 test 2.3041 metric ['0.0976']\n",
      "100%|██████████| 938/938 [00:37<00:00, 24.87it/s]\n",
      "2022-12-23 19:41:59.109 | INFO     | __main__:trainloop:68 - Epoch 4 train 2.3034 test 2.3036 metric ['0.0977']\n",
      "100%|██████████| 5/5 [03:17<00:00, 39.55s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 2\n",
    "model = CNN_2_4().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:26:58.578 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2026\n",
      "100%|██████████| 938/938 [00:28<00:00, 33.09it/s]\n",
      "2022-12-23 20:27:29.220 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.5151 test 0.4004 metric ['0.8538']\n",
      "100%|██████████| 938/938 [00:27<00:00, 33.78it/s]\n",
      "2022-12-23 20:27:59.003 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.3625 test 0.3741 metric ['0.8586']\n",
      "100%|██████████| 938/938 [00:27<00:00, 33.50it/s]\n",
      "2022-12-23 20:28:29.002 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3331 test 0.3514 metric ['0.8773']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.00it/s]\n",
      "2022-12-23 20:28:58.594 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3155 test 0.3594 metric ['0.8745']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.27it/s]\n",
      "2022-12-23 20:29:27.975 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3089 test 0.3358 metric ['0.8777']\n",
      "100%|██████████| 5/5 [02:29<00:00, 29.83s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 2\n",
    "model = CNN_3_2().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:23:29.482 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2023\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.09it/s]\n",
      "2022-12-23 20:23:59.451 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.6562 test 0.4600 metric ['0.8295']\n",
      "100%|██████████| 938/938 [00:27<00:00, 33.90it/s]\n",
      "2022-12-23 20:24:29.424 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4177 test 0.4083 metric ['0.8400']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.11it/s]\n",
      "2022-12-23 20:24:59.089 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3838 test 0.3833 metric ['0.8620']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.25it/s]\n",
      "2022-12-23 20:25:28.702 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3669 test 0.3738 metric ['0.8644']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.01it/s]\n",
      "2022-12-23 20:25:58.515 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3488 test 0.3868 metric ['0.8685']\n",
      "100%|██████████| 5/5 [02:28<00:00, 29.77s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 2\n",
    "model = CNN_3_4().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:30:08.541 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2030\n",
      "100%|██████████| 938/938 [00:22<00:00, 41.37it/s]\n",
      "2022-12-23 20:30:33.078 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.4410 test 0.3709 metric ['0.8652']\n",
      "100%|██████████| 938/938 [00:24<00:00, 38.41it/s]\n",
      "2022-12-23 20:30:59.224 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.3130 test 0.3302 metric ['0.8720']\n",
      "100%|██████████| 938/938 [00:23<00:00, 39.16it/s]\n",
      "2022-12-23 20:31:24.929 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.2799 test 0.3404 metric ['0.8717']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.12it/s]\n",
      "2022-12-23 20:31:51.954 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.2660 test 0.3215 metric ['0.8888']\n",
      "100%|██████████| 938/938 [00:24<00:00, 38.03it/s]\n",
      "2022-12-23 20:32:18.274 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.2525 test 0.3535 metric ['0.8834']\n",
      "100%|██████████| 5/5 [02:09<00:00, 25.92s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_2_2_16_32().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:32:18.486 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2032\n",
      "100%|██████████| 938/938 [00:56<00:00, 16.72it/s]\n",
      "2022-12-23 20:33:17.621 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.5716 test 0.4605 metric ['0.8354']\n",
      "100%|██████████| 938/938 [01:02<00:00, 14.96it/s]\n",
      "2022-12-23 20:34:23.149 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4140 test 0.4331 metric ['0.8426']\n",
      "100%|██████████| 938/938 [01:02<00:00, 15.04it/s]\n",
      "2022-12-23 20:35:28.342 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3866 test 0.4230 metric ['0.8499']\n",
      "100%|██████████| 938/938 [01:01<00:00, 15.21it/s]\n",
      "2022-12-23 20:36:32.745 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3685 test 0.4421 metric ['0.8454']\n",
      "100%|██████████| 938/938 [01:02<00:00, 15.07it/s]\n",
      "2022-12-23 20:37:37.763 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3536 test 0.3925 metric ['0.8585']\n",
      "100%|██████████| 5/5 [05:19<00:00, 63.82s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_2_2_32_64().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:37:59.697 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2037\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.49it/s]\n",
      "2022-12-23 20:38:26.583 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.5729 test 0.4638 metric ['0.8384']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.09it/s]\n",
      "2022-12-23 20:38:55.798 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4122 test 0.3926 metric ['0.8533']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.56it/s]\n",
      "2022-12-23 20:39:24.644 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3777 test 0.3874 metric ['0.8570']\n",
      "100%|██████████| 938/938 [00:26<00:00, 34.97it/s]\n",
      "2022-12-23 20:39:53.167 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3640 test 0.4022 metric ['0.8553']\n",
      "100%|██████████| 938/938 [00:26<00:00, 34.86it/s]\n",
      "2022-12-23 20:40:21.781 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3538 test 0.4140 metric ['0.8527']\n",
      "100%|██████████| 5/5 [02:21<00:00, 28.39s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_2_3_16_32().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:41:44.071 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2041\n",
      "100%|██████████| 938/938 [01:01<00:00, 15.35it/s]\n",
      "2022-12-23 20:42:48.247 | INFO     | __main__:trainloop:68 - Epoch 0 train 2.3316 test 2.3048 metric ['0.0957']\n",
      "100%|██████████| 938/938 [01:11<00:00, 13.15it/s]\n",
      "2022-12-23 20:44:02.446 | INFO     | __main__:trainloop:68 - Epoch 1 train 2.3035 test 2.3037 metric ['0.1026']\n",
      "100%|██████████| 938/938 [01:10<00:00, 13.34it/s]\n",
      "2022-12-23 20:45:15.584 | INFO     | __main__:trainloop:68 - Epoch 2 train 2.3035 test 2.3036 metric ['0.0983']\n",
      "100%|██████████| 938/938 [01:10<00:00, 13.40it/s]\n",
      "2022-12-23 20:46:28.534 | INFO     | __main__:trainloop:68 - Epoch 3 train 2.3034 test 2.3033 metric ['0.1016']\n",
      "100%|██████████| 938/938 [01:09<00:00, 13.42it/s]\n",
      "2022-12-23 20:47:41.244 | INFO     | __main__:trainloop:68 - Epoch 4 train 2.3035 test 2.3037 metric ['0.0993']\n",
      "100%|██████████| 5/5 [05:56<00:00, 71.40s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_2_3_32_64().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 21:03:27.623 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2103\n",
      "100%|██████████| 938/938 [02:58<00:00,  5.24it/s]\n",
      "2022-12-23 21:06:32.954 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.9000 test 0.4390 metric ['0.8394']\n",
      "100%|██████████| 938/938 [03:35<00:00,  4.35it/s]\n",
      "2022-12-23 21:10:14.508 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4217 test 0.4390 metric ['0.8444']\n",
      "100%|██████████| 938/938 [03:34<00:00,  4.38it/s]\n",
      "2022-12-23 21:13:54.582 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3980 test 0.4082 metric ['0.8496']\n",
      "100%|██████████| 938/938 [03:33<00:00,  4.40it/s]\n",
      "2022-12-23 21:17:33.435 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3900 test 0.3889 metric ['0.8567']\n",
      "100%|██████████| 938/938 [03:33<00:00,  4.39it/s]\n",
      "2022-12-23 21:21:12.949 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3773 test 0.4120 metric ['0.8533']\n",
      "100%|██████████| 5/5 [17:45<00:00, 213.01s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_2_3_64_128().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 08:17:47.609 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221224-0817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given groups=1, weight of size [32, 16, 3, 3], expected input[64, 32, 6, 6] to have 16 channels, but got 32 channels instead\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 16, 3, 3], expected input[64, 32, 6, 6] to have 16 channels, but got 32 channels instead\n  In call to configurable 'trainloop' (<function trainloop at 0x7f9f9fc0f940>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Stap 3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m CNN_3_2_16_32()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m      5\u001b[0m                 model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      6\u001b[0m                 metrics\u001b[39m=\u001b[39;49m[accuracy],\n\u001b[1;32m      7\u001b[0m                 train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      8\u001b[0m                 test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m      9\u001b[0m                 train_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_dataloader),\n\u001b[1;32m     10\u001b[0m                 eval_steps\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m                 )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m     38\u001b[0m         grid \u001b[39m=\u001b[39m make_grid(images)\n\u001b[1;32m     39\u001b[0m         writer\u001b[39m.\u001b[39madd_image(\u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m, grid)\n\u001b[0;32m---> 40\u001b[0m     writer\u001b[39m.\u001b[39;49madd_graph(model, images)\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     43\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m     44\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m     45\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    837\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mtensorboard.logging.add_graph\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    839\u001b[0m     \u001b[39m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m         graph(model, input_to_model, verbose, use_strict_trace)\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[39m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:343\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    342\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError occurs, No graph saved\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 343\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    346\u001b[0m     \u001b[39mprint\u001b[39m(graph)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    336\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m         trace \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model, args, strict\u001b[39m=\u001b[39;49muse_strict_trace)\n\u001b[1;32m    338\u001b[0m         graph \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mgraph\n\u001b[1;32m    339\u001b[0m         torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/jit/_trace.py:759\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    760\u001b[0m         func,\n\u001b[1;32m    761\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    762\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    763\u001b[0m         check_trace,\n\u001b[1;32m    764\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    765\u001b[0m         check_tolerance,\n\u001b[1;32m    766\u001b[0m         strict,\n\u001b[1;32m    767\u001b[0m         _force_outplace,\n\u001b[1;32m    768\u001b[0m         _module_class,\n\u001b[1;32m    769\u001b[0m     )\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    772\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    773\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    774\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    777\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    778\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m         _module_class,\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/jit/_trace.py:976\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    972\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    974\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 976\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    977\u001b[0m     method_name,\n\u001b[1;32m    978\u001b[0m     func,\n\u001b[1;32m    979\u001b[0m     example_inputs,\n\u001b[1;32m    980\u001b[0m     var_lookup_fn,\n\u001b[1;32m    981\u001b[0m     strict,\n\u001b[1;32m    982\u001b[0m     _force_outplace,\n\u001b[1;32m    983\u001b[0m     argument_names,\n\u001b[1;32m    984\u001b[0m )\n\u001b[1;32m    985\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    987\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m, in \u001b[0;36mCNN_3_2_16_32.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvolutions(x)\n\u001b[1;32m     31\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(x)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 16, 3, 3], expected input[64, 32, 6, 6] to have 16 channels, but got 32 channels instead\n  In call to configurable 'trainloop' (<function trainloop at 0x7f9f9fc0f940>)"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_3_2_16_32().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 08:03:04.557 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221224-0803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given groups=1, weight of size [64, 32, 3, 3], expected input[64, 64, 6, 6] to have 32 channels, but got 64 channels instead\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 32, 3, 3], expected input[64, 64, 6, 6] to have 32 channels, but got 64 channels instead\n  In call to configurable 'trainloop' (<function trainloop at 0x7f51d073d9d0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Stap 3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m CNN_3_2_32_64()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m      5\u001b[0m                 model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      6\u001b[0m                 metrics\u001b[39m=\u001b[39;49m[accuracy],\n\u001b[1;32m      7\u001b[0m                 train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      8\u001b[0m                 test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m      9\u001b[0m                 train_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_dataloader),\n\u001b[1;32m     10\u001b[0m                 eval_steps\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m                 )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m     38\u001b[0m         grid \u001b[39m=\u001b[39m make_grid(images)\n\u001b[1;32m     39\u001b[0m         writer\u001b[39m.\u001b[39madd_image(\u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m, grid)\n\u001b[0;32m---> 40\u001b[0m     writer\u001b[39m.\u001b[39;49madd_graph(model, images)\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     43\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m     44\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m     45\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    837\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mtensorboard.logging.add_graph\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    839\u001b[0m     \u001b[39m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m         graph(model, input_to_model, verbose, use_strict_trace)\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[39m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:343\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    342\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError occurs, No graph saved\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 343\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    346\u001b[0m     \u001b[39mprint\u001b[39m(graph)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    336\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m         trace \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model, args, strict\u001b[39m=\u001b[39;49muse_strict_trace)\n\u001b[1;32m    338\u001b[0m         graph \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mgraph\n\u001b[1;32m    339\u001b[0m         torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/jit/_trace.py:759\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    760\u001b[0m         func,\n\u001b[1;32m    761\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    762\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    763\u001b[0m         check_trace,\n\u001b[1;32m    764\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    765\u001b[0m         check_tolerance,\n\u001b[1;32m    766\u001b[0m         strict,\n\u001b[1;32m    767\u001b[0m         _force_outplace,\n\u001b[1;32m    768\u001b[0m         _module_class,\n\u001b[1;32m    769\u001b[0m     )\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    772\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    773\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    774\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    777\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    778\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m         _module_class,\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/jit/_trace.py:976\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    972\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    974\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 976\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    977\u001b[0m     method_name,\n\u001b[1;32m    978\u001b[0m     func,\n\u001b[1;32m    979\u001b[0m     example_inputs,\n\u001b[1;32m    980\u001b[0m     var_lookup_fn,\n\u001b[1;32m    981\u001b[0m     strict,\n\u001b[1;32m    982\u001b[0m     _force_outplace,\n\u001b[1;32m    983\u001b[0m     argument_names,\n\u001b[1;32m    984\u001b[0m )\n\u001b[1;32m    985\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    987\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mCNN_3_2_32_64.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 30\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvolutions(x)\n\u001b[1;32m     31\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(x)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 32, 3, 3], expected input[64, 64, 6, 6] to have 32 channels, but got 64 channels instead\n  In call to configurable 'trainloop' (<function trainloop at 0x7f51d073d9d0>)"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_3_2_32_64().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'num_classes', 'kernel_size', 'filter1', and 'filter2'\n  No values supplied by Gin or caller for arguments: ['filter1', 'filter2', 'kernel_size', 'num_classes']\n  Gin had values bound for: []\n  Caller supplied values for: ['self']\n  In call to configurable 'CNN_3_3_16_32' (<class '__main__.CNN_3_3_16_32'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Stap 3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m CNN_3_3_16_32()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m      5\u001b[0m                 model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      6\u001b[0m                 metrics\u001b[39m=\u001b[39m[accuracy],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 eval_steps\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m,\n\u001b[1;32m     11\u001b[0m                 )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'num_classes', 'kernel_size', 'filter1', and 'filter2'\n  No values supplied by Gin or caller for arguments: ['filter1', 'filter2', 'kernel_size', 'num_classes']\n  Gin had values bound for: []\n  Caller supplied values for: ['self']\n  In call to configurable 'CNN_3_3_16_32' (<class '__main__.CNN_3_3_16_32'>)"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_3_3_16_32().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 08:08:36.214 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221224-0808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given groups=1, weight of size [64, 32, 3, 3], expected input[64, 64, 6, 6] to have 32 channels, but got 64 channels instead\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 32, 3, 3], expected input[64, 64, 6, 6] to have 32 channels, but got 64 channels instead\n  In call to configurable 'trainloop' (<function trainloop at 0x7f901b20f940>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Stap 3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m CNN_3_3_32_64()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m      5\u001b[0m                 model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      6\u001b[0m                 metrics\u001b[39m=\u001b[39;49m[accuracy],\n\u001b[1;32m      7\u001b[0m                 train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      8\u001b[0m                 test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m      9\u001b[0m                 train_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_dataloader),\n\u001b[1;32m     10\u001b[0m                 eval_steps\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m                 )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[1;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m     38\u001b[0m         grid \u001b[39m=\u001b[39m make_grid(images)\n\u001b[1;32m     39\u001b[0m         writer\u001b[39m.\u001b[39madd_image(\u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m, grid)\n\u001b[0;32m---> 40\u001b[0m     writer\u001b[39m.\u001b[39;49madd_graph(model, images)\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     43\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m     44\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m     45\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    837\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mtensorboard.logging.add_graph\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    839\u001b[0m     \u001b[39m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m         graph(model, input_to_model, verbose, use_strict_trace)\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[39m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcaffe2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:343\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    342\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError occurs, No graph saved\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 343\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    346\u001b[0m     \u001b[39mprint\u001b[39m(graph)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    336\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m         trace \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model, args, strict\u001b[39m=\u001b[39;49muse_strict_trace)\n\u001b[1;32m    338\u001b[0m         graph \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mgraph\n\u001b[1;32m    339\u001b[0m         torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/jit/_trace.py:759\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    760\u001b[0m         func,\n\u001b[1;32m    761\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    762\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    763\u001b[0m         check_trace,\n\u001b[1;32m    764\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    765\u001b[0m         check_tolerance,\n\u001b[1;32m    766\u001b[0m         strict,\n\u001b[1;32m    767\u001b[0m         _force_outplace,\n\u001b[1;32m    768\u001b[0m         _module_class,\n\u001b[1;32m    769\u001b[0m     )\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    772\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    773\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    774\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    777\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    778\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    785\u001b[0m         _module_class,\n\u001b[1;32m    786\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/jit/_trace.py:976\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    972\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    974\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 976\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    977\u001b[0m     method_name,\n\u001b[1;32m    978\u001b[0m     func,\n\u001b[1;32m    979\u001b[0m     example_inputs,\n\u001b[1;32m    980\u001b[0m     var_lookup_fn,\n\u001b[1;32m    981\u001b[0m     strict,\n\u001b[1;32m    982\u001b[0m     _force_outplace,\n\u001b[1;32m    983\u001b[0m     argument_names,\n\u001b[1;32m    984\u001b[0m )\n\u001b[1;32m    985\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    987\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[23], line 32\u001b[0m, in \u001b[0;36mCNN_3_3_32_64.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 32\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvolutions(x)\n\u001b[1;32m     33\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(x)\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 32, 3, 3], expected input[64, 64, 6, 6] to have 32 channels, but got 64 channels instead\n  In call to configurable 'trainloop' (<function trainloop at 0x7f901b20f940>)"
     ]
    }
   ],
   "source": [
    "## Stap 3\n",
    "model = CNN_3_3_32_64().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 20:48:10.673 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2048\n",
      "100%|██████████| 938/938 [02:31<00:00,  6.21it/s]\n",
      "2022-12-23 20:50:47.767 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.6470 test 0.4424 metric ['0.8352']\n",
      "100%|██████████| 938/938 [02:59<00:00,  5.23it/s]\n",
      "2022-12-23 20:53:52.921 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4093 test 0.4006 metric ['0.8491']\n",
      "100%|██████████| 938/938 [02:59<00:00,  5.21it/s]\n",
      "2022-12-23 20:56:58.188 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3823 test 0.4303 metric ['0.8507']\n",
      "100%|██████████| 938/938 [02:59<00:00,  5.23it/s]\n",
      "2022-12-23 21:00:03.043 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3753 test 0.3859 metric ['0.8596']\n",
      "100%|██████████| 938/938 [02:59<00:00,  5.21it/s]\n",
      "2022-12-23 21:03:08.391 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3629 test 0.3986 metric ['0.8539']\n",
      "100%|██████████| 5/5 [14:57<00:00, 179.49s/it]\n"
     ]
    }
   ],
   "source": [
    "## stap 3 Nieuw\n",
    "model = CNN_2_2_64_128().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 07:27:33.615 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221224-0727\n",
      "100%|██████████| 938/938 [00:58<00:00, 16.00it/s]\n",
      "2022-12-24 07:28:35.166 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.6572 test 0.5043 metric ['0.8157']\n",
      "100%|██████████| 938/938 [01:08<00:00, 13.69it/s]\n",
      "2022-12-24 07:29:46.510 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4505 test 0.4547 metric ['0.8222']\n",
      "100%|██████████| 938/938 [01:08<00:00, 13.72it/s]\n",
      "2022-12-24 07:30:57.559 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.4240 test 0.4333 metric ['0.8523']\n",
      "100%|██████████| 938/938 [01:07<00:00, 13.81it/s]\n",
      "2022-12-24 07:32:08.244 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.4075 test 0.4399 metric ['0.8529']\n",
      "100%|██████████| 938/938 [01:08<00:00, 13.75it/s]\n",
      "2022-12-24 07:33:19.177 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3965 test 0.5124 metric ['0.8167']\n",
      "100%|██████████| 5/5 [05:45<00:00, 69.08s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3 Nieuw\n",
    "model = CNN_3_4_16_32().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 21:21:55.969 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221223-2121\n",
      "100%|██████████| 938/938 [00:58<00:00, 15.99it/s]\n",
      "2022-12-23 21:22:57.759 | INFO     | __main__:trainloop:68 - Epoch 0 train 2.3266 test 2.3040 metric ['0.1028']\n",
      "100%|██████████| 938/938 [01:07<00:00, 13.97it/s]\n",
      "2022-12-23 21:24:07.827 | INFO     | __main__:trainloop:68 - Epoch 1 train 2.3037 test 2.3033 metric ['0.1024']\n",
      "100%|██████████| 938/938 [01:07<00:00, 13.99it/s]\n",
      "2022-12-23 21:25:17.753 | INFO     | __main__:trainloop:68 - Epoch 2 train 2.3032 test 2.3031 metric ['0.1032']\n",
      "100%|██████████| 938/938 [01:06<00:00, 14.17it/s]\n",
      "2022-12-23 21:26:26.941 | INFO     | __main__:trainloop:68 - Epoch 3 train 2.3034 test 2.3043 metric ['0.1001']\n",
      "100%|██████████| 938/938 [01:06<00:00, 14.16it/s]\n",
      "2022-12-23 21:27:36.028 | INFO     | __main__:trainloop:68 - Epoch 4 train 2.3038 test 2.3032 metric ['0.1016']\n",
      "100%|██████████| 5/5 [05:39<00:00, 67.98s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3 Nieuw\n",
    "model = CNN_3_4_32_64().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-24 07:38:31.702 | INFO     | __main__:dir_add_timestamp:39 - Logging to /home/admindme/code/ML22_opdracht1/models/test/20221224-0738\n",
      "100%|██████████| 938/938 [02:50<00:00,  5.49it/s]\n",
      "2022-12-24 07:41:28.430 | INFO     | __main__:trainloop:68 - Epoch 0 train 2.4089 test 2.3039 metric ['0.0939']\n",
      "100%|██████████| 938/938 [03:27<00:00,  4.52it/s]\n",
      "2022-12-24 07:45:01.406 | INFO     | __main__:trainloop:68 - Epoch 1 train 2.3035 test 2.3034 metric ['0.0940']\n",
      "100%|██████████| 938/938 [03:27<00:00,  4.52it/s]\n",
      "2022-12-24 07:48:34.520 | INFO     | __main__:trainloop:68 - Epoch 2 train 2.3034 test 2.3038 metric ['0.0933']\n",
      "100%|██████████| 938/938 [03:28<00:00,  4.51it/s]\n",
      "2022-12-24 07:52:08.143 | INFO     | __main__:trainloop:68 - Epoch 3 train 2.3033 test 2.3047 metric ['0.1014']\n",
      "100%|██████████| 938/938 [03:26<00:00,  4.54it/s]\n",
      "2022-12-24 07:55:40.430 | INFO     | __main__:trainloop:68 - Epoch 4 train 2.3036 test 2.3030 metric ['0.1014']\n",
      "100%|██████████| 5/5 [17:08<00:00, 205.67s/it]\n"
     ]
    }
   ],
   "source": [
    "## Stap 3 Nieuw\n",
    "model = CNN_3_4_64_128().to(device)\n",
    "\n",
    "model = trainloop(\n",
    "                model=model,\n",
    "                metrics=[accuracy],\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                train_steps=len(train_dataloader),\n",
    "                eval_steps=150,\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-mwI8DU6x-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5829ab1a7438afd0eb3e39a540a9bafca59bf334debab56f2c37d99237ff203a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
