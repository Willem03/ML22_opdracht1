{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 17:55:26.969653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-17 17:55:28.051463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-17 17:55:28.051567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-17 17:55:28.051577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, \"../..\")\n",
    "\n",
    "from typing import Callable, Protocol, Dict, Optional, Iterator, List, Tuple\n",
    "\n",
    "import gin\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#from src.data import make_dataset\n",
    "#from src.models import metrics\n",
    "#from src.models import train_model # om de functie train_loop binnen te halen\n",
    "\n",
    "# ------------------------------------------------------------------------------- #\n",
    "## Alle imports benodigd voor de functie train_loop uit train_model.py \n",
    "\n",
    "import tensorflow as tf  # noqa: F401\n",
    "\n",
    "# needed to make summarywriter load without error\n",
    "from loguru import logger\n",
    "from numpy import Inf\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from src.models.metrics import Metric\n",
    "#from src.typehinting import GenericModel\n",
    "#from src.data import data_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benodigde functie uitmake_dataset.py\n",
    "\n",
    "#@gin.configurable\n",
    "def get_MNIST(  # noqa: N802\n",
    "    data_dir: Path, batch_size: int\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benodigde funties uit metrics.py\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class Metric:\n",
    "    def __repr__(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Accuracy\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        return (yhat.argmax(dim=1) == y).sum() / len(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericModel(Protocol):\n",
    "    train: Callable\n",
    "    eval: Callable\n",
    "    parameters: Callable\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> torch.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: GenericModel) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit data_tools.py\n",
    "\n",
    "def dir_add_timestamp(log_dir: Optional[Path] = None) -> Path:\n",
    "    if log_dir is None:\n",
    "        log_dir = Path(\".\")\n",
    "    log_dir = Path(log_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    log_dir = log_dir / timestamp\n",
    "    logger.info(f\"Logging to {log_dir}\")\n",
    "    if not log_dir.exists():\n",
    "        log_dir.mkdir(parents=True)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren functie train_loop\n",
    "\n",
    "#@gin.configurable\n",
    "def trainloop(\n",
    "    epochs: int,\n",
    "    model: GenericModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    learning_rate: float,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    train_dataloader: Iterator,\n",
    "    test_dataloader: Iterator,\n",
    "    log_dir: Path,\n",
    "    train_steps: int,\n",
    "    eval_steps: int,\n",
    "    patience: int = 10,\n",
    "    factor: float = 0.9,\n",
    "    tunewriter: bool = False,\n",
    ") -> GenericModel:\n",
    "    \n",
    "    optimizer_: torch.optim.Optimizer = optimizer(\n",
    "        model.parameters(), lr=learning_rate\n",
    "    )  # type: ignore\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_,\n",
    "        factor=factor,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    if not tunewriter:\n",
    "        log_dir = dir_add_timestamp(log_dir)\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        #write_gin(log_dir, gin.config_str())\n",
    "\n",
    "        images, _ = next(iter(train_dataloader))\n",
    "        if len(images.shape) == 4:\n",
    "            grid = make_grid(images)\n",
    "            writer.add_image(\"images\", grid)\n",
    "        writer.add_graph(model, images)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = trainbatches(\n",
    "            model, train_dataloader, loss_fn, optimizer_, train_steps\n",
    "        )\n",
    "\n",
    "        metric_dict, test_loss = evalbatches(\n",
    "            model, test_dataloader, loss_fn, metrics, eval_steps\n",
    "        )\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        if tunewriter:\n",
    "            tune.report(\n",
    "                iterations=epoch,\n",
    "                train_loss=train_loss,\n",
    "                test_loss=test_loss,\n",
    "                **metric_dict,\n",
    "            )\n",
    "        else:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "            for m in metric_dict:\n",
    "                writer.add_scalar(f\"metric/{m}\", metric_dict[m], epoch)\n",
    "            lr = [group[\"lr\"] for group in optimizer_.param_groups][0]\n",
    "            writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "            metric_scores = [f\"{v:.4f}\" for v in metric_dict.values()]\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} train {train_loss:.4f} test {test_loss:.4f} metric {metric_scores}\"  # noqa E501\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit train_model.py\n",
    "\n",
    "def trainbatches(\n",
    "    model: GenericModel,\n",
    "    traindatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_steps: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss: float = 0.0\n",
    "    for _ in tqdm(range(train_steps)):\n",
    "        x, y = next(iter(traindatastreamer))\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.detach().numpy()\n",
    "    train_loss /= train_steps\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit train_model.py\n",
    "\n",
    "def evalbatches(\n",
    "    model: GenericModel,\n",
    "    testdatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    eval_steps: int,\n",
    ") -> Tuple[Dict[str, float], float]:\n",
    "    model.eval()\n",
    "    test_loss: float = 0.0\n",
    "    metric_dict: Dict[str, float] = {}\n",
    "    for _ in range(eval_steps):\n",
    "        x, y = next(iter(testdatastreamer))\n",
    "        yhat = model(x)\n",
    "        test_loss += loss_fn(yhat, y).detach().numpy()\n",
    "        for m in metrics:\n",
    "            metric_dict[str(m)] = (\n",
    "                metric_dict.get(str(m), 0.0) + m(y, yhat).detach().numpy()\n",
    "            )\n",
    "\n",
    "    test_loss /= eval_steps\n",
    "    for key in metric_dict:\n",
    "        metric_dict[str(key)] = metric_dict[str(key)] / eval_steps\n",
    "    return metric_dict, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 1: CNN_een met een convolutional layer & drie linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_een_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 3136),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3136, 1568),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1568, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_een_drie = CNN_een_drie(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2: CNN_twee met twee convolutional layers & twee linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee_twee(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_twee_twee = CNN_twee_twee(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.3: CNN_twee met twee convolutional layers en drie linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_twee_drie = CNN_twee_drie(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.4: CNN_twee met twee convolutional layers & vier linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee_vier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_twee_vier = CNN_twee_vier(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.2: CNN_drie met twee convolutional layers & twee linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie_twee(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_drie_twee = CNN_drie_twee(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.3: CNN_drie met drie convolutional layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_drie_drie = CNN_drie_drie(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4: CNN_drie met drie convolutional layers & vier linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie_vier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_drie_vier = CNN_drie_vier(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellen = [\n",
    "            model_een_drie,\n",
    "            model_twee_twee,\n",
    "            model_twee_drie,\n",
    "            model_twee_vier,\n",
    "            model_drie_twee,\n",
    "            model_drie_drie,\n",
    "            model_drie_vier,\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in modellen:\n",
    "    print(summary(model, input_size=(64, 1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir =  Path(\"/home/admindme/code/ML22_opdracht1/data/raw/FashionMNIST\")\n",
    "train_dataloader, test_dataloader = get_MNIST(datadir, 64)\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 18:41:34.656 | INFO     | __main__:dir_add_timestamp:9 - Logging to ../../models/test/20221217-1841\n",
      "100%|██████████| 938/938 [00:29<00:00, 31.95it/s]\n",
      "2022-12-17 18:42:06.491 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.7348 test 0.5453 metric ['0.7887']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.80it/s]\n",
      "2022-12-17 18:42:39.371 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4631 test 0.4382 metric ['0.8363']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.05it/s]\n",
      "2022-12-17 18:43:11.918 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3735 test 0.3568 metric ['0.8680']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.95it/s]\n",
      "2022-12-17 18:43:45.635 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3258 test 0.3278 metric ['0.8862']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.69it/s]\n",
      "2022-12-17 18:44:19.612 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3058 test 0.3068 metric ['0.8888']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.97it/s]\n",
      "2022-12-17 18:44:52.236 | INFO     | __main__:trainloop:68 - Epoch 5 train 0.2816 test 0.3287 metric ['0.8781']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.33it/s]\n",
      "2022-12-17 18:45:25.538 | INFO     | __main__:trainloop:68 - Epoch 6 train 0.2653 test 0.3033 metric ['0.8922']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.54it/s]\n",
      "2022-12-17 18:45:58.517 | INFO     | __main__:trainloop:68 - Epoch 7 train 0.2537 test 0.3032 metric ['0.8921']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.16it/s]\n",
      "2022-12-17 18:46:30.933 | INFO     | __main__:trainloop:68 - Epoch 8 train 0.2410 test 0.2967 metric ['0.8962']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.02it/s]\n",
      "2022-12-17 18:47:03.524 | INFO     | __main__:trainloop:68 - Epoch 9 train 0.2353 test 0.2769 metric ['0.9015']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.28it/s]\n",
      "2022-12-17 18:47:36.853 | INFO     | __main__:trainloop:68 - Epoch 10 train 0.2178 test 0.2756 metric ['0.8998']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.36it/s]\n",
      "2022-12-17 18:48:10.067 | INFO     | __main__:trainloop:68 - Epoch 11 train 0.2148 test 0.2649 metric ['0.9062']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.50it/s]\n",
      "2022-12-17 18:48:43.115 | INFO     | __main__:trainloop:68 - Epoch 12 train 0.2065 test 0.2663 metric ['0.9099']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.69it/s]\n",
      "2022-12-17 18:49:15.950 | INFO     | __main__:trainloop:68 - Epoch 13 train 0.2055 test 0.2867 metric ['0.8983']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.79it/s]\n",
      "2022-12-17 18:49:48.830 | INFO     | __main__:trainloop:68 - Epoch 14 train 0.1933 test 0.2715 metric ['0.9064']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.19it/s]\n",
      "2022-12-17 18:50:22.240 | INFO     | __main__:trainloop:68 - Epoch 15 train 0.1863 test 0.2895 metric ['0.9000']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.98it/s]\n",
      "2022-12-17 18:50:54.873 | INFO     | __main__:trainloop:68 - Epoch 16 train 0.1796 test 0.2675 metric ['0.9039']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.58it/s]\n",
      "2022-12-17 18:51:27.873 | INFO     | __main__:trainloop:68 - Epoch 17 train 0.1777 test 0.2865 metric ['0.9028']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.24it/s]\n",
      "2022-12-17 18:52:01.250 | INFO     | __main__:trainloop:68 - Epoch 18 train 0.1747 test 0.2858 metric ['0.8981']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.25it/s]\n",
      "2022-12-17 18:52:35.697 | INFO     | __main__:trainloop:68 - Epoch 19 train 0.1684 test 0.2977 metric ['0.8952']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.45it/s]\n",
      "2022-12-17 18:53:09.887 | INFO     | __main__:trainloop:68 - Epoch 20 train 0.1644 test 0.2694 metric ['0.9102']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.22it/s]\n",
      "2022-12-17 18:53:44.354 | INFO     | __main__:trainloop:68 - Epoch 21 train 0.1583 test 0.2776 metric ['0.9070']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.10it/s]\n",
      "2022-12-17 18:54:18.940 | INFO     | __main__:trainloop:68 - Epoch 22 train 0.1516 test 0.2693 metric ['0.9174']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.70it/s]\n",
      "2022-12-17 18:54:52.847 | INFO     | __main__:trainloop:68 - Epoch 23 train 0.1465 test 0.2639 metric ['0.9139']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.44it/s]\n",
      "2022-12-17 18:55:25.922 | INFO     | __main__:trainloop:68 - Epoch 24 train 0.1395 test 0.3027 metric ['0.9001']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.88it/s]\n",
      "2022-12-17 18:55:59.645 | INFO     | __main__:trainloop:68 - Epoch 25 train 0.1418 test 0.3040 metric ['0.9069']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.36it/s]\n",
      "2022-12-17 18:56:32.830 | INFO     | __main__:trainloop:68 - Epoch 26 train 0.1382 test 0.2954 metric ['0.9078']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.54it/s]\n",
      "2022-12-17 18:57:05.792 | INFO     | __main__:trainloop:68 - Epoch 27 train 0.1347 test 0.2739 metric ['0.9106']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.00it/s]\n",
      "2022-12-17 18:57:38.326 | INFO     | __main__:trainloop:68 - Epoch 28 train 0.1289 test 0.2880 metric ['0.9111']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.15it/s]\n",
      "2022-12-17 18:58:11.752 | INFO     | __main__:trainloop:68 - Epoch 29 train 0.1226 test 0.2811 metric ['0.9078']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.45it/s]\n",
      "2022-12-17 18:58:44.778 | INFO     | __main__:trainloop:68 - Epoch 30 train 0.1204 test 0.2838 metric ['0.9146']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.66it/s]\n",
      "2022-12-17 18:59:17.622 | INFO     | __main__:trainloop:68 - Epoch 31 train 0.1223 test 0.2828 metric ['0.9179']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.78it/s]\n",
      "2022-12-17 18:59:50.431 | INFO     | __main__:trainloop:68 - Epoch 32 train 0.1172 test 0.3091 metric ['0.9068']\n",
      "100%|██████████| 938/938 [00:29<00:00, 31.48it/s]\n",
      "2022-12-17 19:00:22.575 | INFO     | __main__:trainloop:68 - Epoch 33 train 0.1117 test 0.2950 metric ['0.9127']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.82it/s]\n",
      "2022-12-17 19:00:55.441 | INFO     | __main__:trainloop:68 - Epoch 34 train 0.1108 test 0.3085 metric ['0.9140']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.05it/s]\n",
      "2022-12-17 19:01:27.964 | INFO     | __main__:trainloop:68 - Epoch 35 train 0.1061 test 0.2982 metric ['0.9111']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.69it/s]\n",
      "2022-12-17 19:02:00.867 | INFO     | __main__:trainloop:68 - Epoch 36 train 0.1050 test 0.3336 metric ['0.9102']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.02it/s]\n",
      "2022-12-17 19:02:34.507 | INFO     | __main__:trainloop:68 - Epoch 37 train 0.0996 test 0.3369 metric ['0.9092']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.75it/s]\n",
      "2022-12-17 19:03:08.262 | INFO     | __main__:trainloop:68 - Epoch 38 train 0.0963 test 0.3521 metric ['0.9077']\n",
      "100%|██████████| 938/938 [00:29<00:00, 31.31it/s]\n",
      "2022-12-17 19:03:40.496 | INFO     | __main__:trainloop:68 - Epoch 39 train 0.0943 test 0.3616 metric ['0.9025']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.50it/s]\n",
      "2022-12-17 19:04:13.370 | INFO     | __main__:trainloop:68 - Epoch 40 train 0.0945 test 0.3455 metric ['0.9061']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.12it/s]\n",
      "2022-12-17 19:04:45.777 | INFO     | __main__:trainloop:68 - Epoch 41 train 0.0949 test 0.3486 metric ['0.9078']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.41it/s]\n",
      "2022-12-17 19:05:18.932 | INFO     | __main__:trainloop:68 - Epoch 42 train 0.0889 test 0.3622 metric ['0.9150']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.36it/s]\n",
      "2022-12-17 19:05:52.106 | INFO     | __main__:trainloop:68 - Epoch 43 train 0.0913 test 0.3438 metric ['0.9119']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.82it/s]\n",
      "2022-12-17 19:06:24.817 | INFO     | __main__:trainloop:68 - Epoch 44 train 0.0852 test 0.3727 metric ['0.9150']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.32it/s]\n",
      "2022-12-17 19:06:58.067 | INFO     | __main__:trainloop:68 - Epoch 45 train 0.0841 test 0.3201 metric ['0.9109']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.67it/s]\n",
      "2022-12-17 19:07:30.792 | INFO     | __main__:trainloop:68 - Epoch 46 train 0.0833 test 0.3768 metric ['0.9076']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.94it/s]\n",
      "2022-12-17 19:08:03.300 | INFO     | __main__:trainloop:68 - Epoch 47 train 0.0760 test 0.3745 metric ['0.9148']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.23it/s]\n",
      "2022-12-17 19:08:35.658 | INFO     | __main__:trainloop:68 - Epoch 48 train 0.0763 test 0.3826 metric ['0.9106']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.65it/s]\n",
      "2022-12-17 19:09:09.538 | INFO     | __main__:trainloop:68 - Epoch 49 train 0.0729 test 0.3798 metric ['0.9154']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.95it/s]\n",
      "2022-12-17 19:09:42.091 | INFO     | __main__:trainloop:68 - Epoch 50 train 0.0767 test 0.3871 metric ['0.9061']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.14it/s]\n",
      "2022-12-17 19:10:14.463 | INFO     | __main__:trainloop:68 - Epoch 51 train 0.0730 test 0.3820 metric ['0.9130']\n",
      "100%|██████████| 938/938 [00:29<00:00, 32.02it/s]\n",
      "2022-12-17 19:10:45.982 | INFO     | __main__:trainloop:68 - Epoch 52 train 0.0713 test 0.4080 metric ['0.9111']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.00it/s]\n",
      "2022-12-17 19:11:19.609 | INFO     | __main__:trainloop:68 - Epoch 53 train 0.0658 test 0.4093 metric ['0.9107']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.68it/s]\n",
      "2022-12-17 19:11:53.558 | INFO     | __main__:trainloop:68 - Epoch 54 train 0.0695 test 0.4173 metric ['0.9103']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.55it/s]\n",
      "2022-12-17 19:12:27.592 | INFO     | __main__:trainloop:68 - Epoch 55 train 0.0668 test 0.4417 metric ['0.9073']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.03it/s]\n",
      "2022-12-17 19:13:01.228 | INFO     | __main__:trainloop:68 - Epoch 56 train 0.0644 test 0.4737 metric ['0.9003']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.66it/s]\n",
      "2022-12-17 19:13:35.163 | INFO     | __main__:trainloop:68 - Epoch 57 train 0.0623 test 0.4505 metric ['0.9064']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.93it/s]\n",
      "2022-12-17 19:14:07.765 | INFO     | __main__:trainloop:68 - Epoch 58 train 0.0575 test 0.4656 metric ['0.9045']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.40it/s]\n",
      "2022-12-17 19:14:40.974 | INFO     | __main__:trainloop:68 - Epoch 59 train 0.0574 test 0.5031 metric ['0.9073']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.53it/s]\n",
      "2022-12-17 19:15:15.082 | INFO     | __main__:trainloop:68 - Epoch 60 train 0.0572 test 0.4404 metric ['0.9116']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.32it/s]\n",
      "2022-12-17 19:15:49.353 | INFO     | __main__:trainloop:68 - Epoch 61 train 0.0548 test 0.5061 metric ['0.9046']\n",
      "100%|██████████| 938/938 [00:30<00:00, 31.05it/s]\n",
      "2022-12-17 19:16:21.782 | INFO     | __main__:trainloop:68 - Epoch 62 train 0.0561 test 0.4936 metric ['0.9116']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.51it/s]\n",
      "2022-12-17 19:16:54.759 | INFO     | __main__:trainloop:68 - Epoch 63 train 0.0521 test 0.4866 metric ['0.9056']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.32it/s]\n",
      "2022-12-17 19:17:27.943 | INFO     | __main__:trainloop:68 - Epoch 64 train 0.0531 test 0.5201 metric ['0.9020']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.82it/s]\n",
      "2022-12-17 19:18:01.723 | INFO     | __main__:trainloop:68 - Epoch 65 train 0.0508 test 0.5347 metric ['0.9051']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.22it/s]\n",
      "2022-12-17 19:18:36.196 | INFO     | __main__:trainloop:68 - Epoch 66 train 0.0523 test 0.4687 metric ['0.9141']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.93it/s]\n",
      "2022-12-17 19:19:09.792 | INFO     | __main__:trainloop:68 - Epoch 67 train 0.0482 test 0.4934 metric ['0.9081']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.70it/s]\n",
      "2022-12-17 19:19:43.771 | INFO     | __main__:trainloop:68 - Epoch 68 train 0.0418 test 0.5142 metric ['0.9075']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.47it/s]\n",
      "2022-12-17 19:20:17.813 | INFO     | __main__:trainloop:68 - Epoch 69 train 0.0422 test 0.5393 metric ['0.9072']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.64it/s]\n",
      "2022-12-17 19:20:50.760 | INFO     | __main__:trainloop:68 - Epoch 70 train 0.0411 test 0.5503 metric ['0.9073']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.40it/s]\n",
      "2022-12-17 19:21:24.937 | INFO     | __main__:trainloop:68 - Epoch 71 train 0.0423 test 0.6100 metric ['0.9044']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.44it/s]\n",
      "2022-12-17 19:21:58.031 | INFO     | __main__:trainloop:68 - Epoch 72 train 0.0425 test 0.5472 metric ['0.9102']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.06it/s]\n",
      "2022-12-17 19:22:32.633 | INFO     | __main__:trainloop:68 - Epoch 73 train 0.0392 test 0.5741 metric ['0.9074']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.13it/s]\n",
      "2022-12-17 19:23:07.202 | INFO     | __main__:trainloop:68 - Epoch 74 train 0.0429 test 0.5813 metric ['0.9052']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.68it/s]\n",
      "2022-12-17 19:23:41.191 | INFO     | __main__:trainloop:68 - Epoch 75 train 0.0382 test 0.5932 metric ['0.9050']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.54it/s]\n",
      "2022-12-17 19:24:15.395 | INFO     | __main__:trainloop:68 - Epoch 76 train 0.0385 test 0.5588 metric ['0.9078']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.95it/s]\n",
      "2022-12-17 19:24:48.993 | INFO     | __main__:trainloop:68 - Epoch 77 train 0.0376 test 0.6178 metric ['0.9031']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.97it/s]\n",
      "2022-12-17 19:25:22.656 | INFO     | __main__:trainloop:68 - Epoch 78 train 0.0350 test 0.6289 metric ['0.9048']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.98it/s]\n",
      "2022-12-17 19:25:56.271 | INFO     | __main__:trainloop:68 - Epoch 79 train 0.0323 test 0.6603 metric ['0.9056']\n",
      "100%|██████████| 938/938 [00:31<00:00, 30.25it/s]\n",
      "2022-12-17 19:26:29.630 | INFO     | __main__:trainloop:68 - Epoch 80 train 0.0307 test 0.6206 metric ['0.9017']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.75it/s]\n",
      "2022-12-17 19:27:02.431 | INFO     | __main__:trainloop:68 - Epoch 81 train 0.0300 test 0.6749 metric ['0.9027']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.76it/s]\n",
      "2022-12-17 19:27:36.274 | INFO     | __main__:trainloop:68 - Epoch 82 train 0.0299 test 0.6711 metric ['0.9104']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.72it/s]\n",
      "2022-12-17 19:28:10.070 | INFO     | __main__:trainloop:68 - Epoch 83 train 0.0330 test 0.6917 metric ['0.9055']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.42it/s]\n",
      "2022-12-17 19:28:43.144 | INFO     | __main__:trainloop:68 - Epoch 84 train 0.0285 test 0.6576 metric ['0.9066']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.60it/s]\n",
      "2022-12-17 19:29:16.026 | INFO     | __main__:trainloop:68 - Epoch 85 train 0.0298 test 0.6664 metric ['0.9040']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.94it/s]\n",
      "2022-12-17 19:29:49.745 | INFO     | __main__:trainloop:68 - Epoch 86 train 0.0283 test 0.7122 metric ['0.9077']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.89it/s]\n",
      "2022-12-17 19:30:23.332 | INFO     | __main__:trainloop:68 - Epoch 87 train 0.0291 test 0.7801 metric ['0.9019']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.68it/s]\n",
      "2022-12-17 19:30:56.197 | INFO     | __main__:trainloop:68 - Epoch 88 train 0.0265 test 0.7309 metric ['0.9031']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.43it/s]\n",
      "2022-12-17 19:31:30.412 | INFO     | __main__:trainloop:68 - Epoch 89 train 0.0286 test 0.7221 metric ['0.9058']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.53it/s]\n",
      "2022-12-17 19:32:04.582 | INFO     | __main__:trainloop:68 - Epoch 90 train 0.0228 test 0.7692 metric ['0.9062']\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.04it/s]\n",
      "2022-12-17 19:32:39.174 | INFO     | __main__:trainloop:68 - Epoch 91 train 0.0229 test 0.6735 metric ['0.9080']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.50it/s]\n",
      "2022-12-17 19:33:13.252 | INFO     | __main__:trainloop:68 - Epoch 92 train 0.0229 test 0.7016 metric ['0.9073']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.94it/s]\n",
      "2022-12-17 19:33:46.887 | INFO     | __main__:trainloop:68 - Epoch 93 train 0.0233 test 0.6821 metric ['0.9081']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.88it/s]\n",
      "2022-12-17 19:34:20.543 | INFO     | __main__:trainloop:68 - Epoch 94 train 0.0237 test 0.7723 metric ['0.9077']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.76it/s]\n",
      "2022-12-17 19:34:54.388 | INFO     | __main__:trainloop:68 - Epoch 95 train 0.0214 test 0.7183 metric ['0.9071']\n",
      "100%|██████████| 938/938 [00:31<00:00, 29.83it/s]\n",
      "2022-12-17 19:35:28.027 | INFO     | __main__:trainloop:68 - Epoch 96 train 0.0207 test 0.6687 metric ['0.9125']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.76it/s]\n",
      "2022-12-17 19:36:00.717 | INFO     | __main__:trainloop:68 - Epoch 97 train 0.0234 test 0.7679 metric ['0.9108']\n",
      "100%|██████████| 938/938 [00:29<00:00, 31.35it/s]\n",
      "2022-12-17 19:36:32.831 | INFO     | __main__:trainloop:68 - Epoch 98 train 0.0186 test 0.8206 metric ['0.9041']\n",
      "100%|██████████| 938/938 [00:30<00:00, 30.35it/s]\n",
      "2022-12-17 19:37:06.042 | INFO     | __main__:trainloop:68 - Epoch 99 train 0.0209 test 0.7976 metric ['0.9025']\n",
      "100%|██████████| 100/100 [55:31<00:00, 33.31s/it]\n"
     ]
    }
   ],
   "source": [
    "model = model_drie_drie\n",
    "epochs = 100\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy()\n",
    "\n",
    "model = trainloop(\n",
    "            epochs=epochs,\n",
    "            model=model,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            learning_rate= 0.001,\n",
    "            loss_fn=loss_fn,\n",
    "            metrics=[accuracy],\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            log_dir=\"../../models/test/\",\n",
    "            train_steps=len(train_dataloader),\n",
    "            eval_steps=150,\n",
    "            #patience= ,\n",
    "            #factor= ,\n",
    "            #tunewriter= ,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-mwI8DU6x-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5829ab1a7438afd0eb3e39a540a9bafca59bf334debab56f2c37d99237ff203a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
