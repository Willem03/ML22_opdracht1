{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, \"../..\")\n",
    "\n",
    "from typing import Callable, Protocol, Dict, Optional, Iterator, List, Tuple\n",
    "\n",
    "import gin\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#from src.data import make_dataset\n",
    "#from src.models import metrics\n",
    "#from src.models import train_model # om de functie train_loop binnen te halen\n",
    "\n",
    "# ------------------------------------------------------------------------------- #\n",
    "## Alle imports benodigd voor de functie train_loop uit train_model.py \n",
    "\n",
    "import tensorflow as tf  # noqa: F401\n",
    "\n",
    "# needed to make summarywriter load without error\n",
    "from loguru import logger\n",
    "from numpy import Inf\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from src.models.metrics import Metric\n",
    "#from src.typehinting import GenericModel\n",
    "#from src.data import data_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benodigde functie uitmake_dataset.py\n",
    "\n",
    "#@gin.configurable\n",
    "def get_MNIST(  # noqa: N802\n",
    "    data_dir: Path, batch_size: int\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benodigde funties uit metrics.py\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class Metric:\n",
    "    def __repr__(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Accuracy\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        return (yhat.argmax(dim=1) == y).sum() / len(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericModel(Protocol):\n",
    "    train: Callable\n",
    "    eval: Callable\n",
    "    parameters: Callable\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> torch.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: GenericModel) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit data_tools.py\n",
    "\n",
    "def dir_add_timestamp(log_dir: Optional[Path] = None) -> Path:\n",
    "    if log_dir is None:\n",
    "        log_dir = Path(\".\")\n",
    "    log_dir = Path(log_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    log_dir = log_dir / timestamp\n",
    "    logger.info(f\"Logging to {log_dir}\")\n",
    "    if not log_dir.exists():\n",
    "        log_dir.mkdir(parents=True)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren functie train_loop\n",
    "\n",
    "#@gin.configurable\n",
    "def trainloop(\n",
    "    epochs: int,\n",
    "    model: GenericModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    learning_rate: float,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    train_dataloader: Iterator,\n",
    "    test_dataloader: Iterator,\n",
    "    log_dir: Path,\n",
    "    train_steps: int,\n",
    "    eval_steps: int,\n",
    "    patience: int = 10,\n",
    "    factor: float = 0.9,\n",
    "    tunewriter: bool = False,\n",
    ") -> GenericModel:\n",
    "    \n",
    "    optimizer_: torch.optim.Optimizer = optimizer(\n",
    "        model.parameters(), lr=learning_rate\n",
    "    )  # type: ignore\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_,\n",
    "        factor=factor,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    if not tunewriter:\n",
    "        log_dir = dir_add_timestamp(log_dir)\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        #write_gin(log_dir, gin.config_str())\n",
    "\n",
    "        images, _ = next(iter(train_dataloader))\n",
    "        if len(images.shape) == 4:\n",
    "            grid = make_grid(images)\n",
    "            writer.add_image(\"images\", grid)\n",
    "        writer.add_graph(model, images)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = trainbatches(\n",
    "            model, train_dataloader, loss_fn, optimizer_, train_steps\n",
    "        )\n",
    "\n",
    "        metric_dict, test_loss = evalbatches(\n",
    "            model, test_dataloader, loss_fn, metrics, eval_steps\n",
    "        )\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        if tunewriter:\n",
    "            tune.report(\n",
    "                iterations=epoch,\n",
    "                train_loss=train_loss,\n",
    "                test_loss=test_loss,\n",
    "                **metric_dict,\n",
    "            )\n",
    "        else:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "            for m in metric_dict:\n",
    "                writer.add_scalar(f\"metric/{m}\", metric_dict[m], epoch)\n",
    "            lr = [group[\"lr\"] for group in optimizer_.param_groups][0]\n",
    "            writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "            metric_scores = [f\"{v:.4f}\" for v in metric_dict.values()]\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} train {train_loss:.4f} test {test_loss:.4f} metric {metric_scores}\"  # noqa E501\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit train_model.py\n",
    "\n",
    "def trainbatches(\n",
    "    model: GenericModel,\n",
    "    traindatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_steps: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss: float = 0.0\n",
    "    for _ in tqdm(range(train_steps)):\n",
    "        x, y = next(iter(traindatastreamer))\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.detach().numpy()\n",
    "    train_loss /= train_steps\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/admindme/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/ray/thirdparty_files',\n",
       " '/home/admindme/code/ML22_opdracht1/notebooks',\n",
       " '/home/admindme/.pyenv/versions/3.9.12/lib/python39.zip',\n",
       " '/home/admindme/.pyenv/versions/3.9.12/lib/python3.9',\n",
       " '/home/admindme/.pyenv/versions/3.9.12/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/admindme/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 1: CNN_een met een convolutional layer\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_een(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 3136),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3136, 1568),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1568, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "\n",
    "model = CNN_een(10, 3, 32, 32).to(device)\n",
    "summary(model, input_size=(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2: CNN_twee met twee convolutional layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "\n",
    "model = CNN_twee(10, 3, 32, 32).to(device)\n",
    "summary(model, input_size=(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3: CNN_drie met drie convolutional layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "\n",
    "model = CNN_drie(10, 3, 32, 32).to(device)\n",
    "summary(model, input_size=(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir =  Path(\"/home/admindme/code/ML22_opdracht1/data/raw/FashionMNIST\")\n",
    "train_dataloader, test_dataloader = get_MNIST(datadir, 64)\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 14:45:50.662 | INFO     | __main__:dir_add_timestamp:9 - Logging to ../../models/test/20221217-1445\n",
      "100%|██████████| 938/938 [00:32<00:00, 29.01it/s]\n",
      "  0%|          | 0/3 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evalbatches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m loss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      5\u001b[0m accuracy \u001b[39m=\u001b[39m Accuracy()\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m      8\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m             model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     10\u001b[0m             optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m     11\u001b[0m             learning_rate\u001b[39m=\u001b[39;49m \u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m             loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     13\u001b[0m             metrics\u001b[39m=\u001b[39;49m[accuracy],\n\u001b[1;32m     14\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     15\u001b[0m             test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m     16\u001b[0m             log_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../../models/test/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m             train_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_dataloader),\n\u001b[1;32m     18\u001b[0m             eval_steps\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m             \u001b[39m#patience= ,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m             \u001b[39m#factor= ,\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m             \u001b[39m#tunewriter= ,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[44], line 47\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     43\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m     44\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 47\u001b[0m     metric_dict, test_loss \u001b[39m=\u001b[39m evalbatches(\n\u001b[1;32m     48\u001b[0m         model, test_dataloader, loss_fn, metrics, eval_steps\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(test_loss)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m tunewriter:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evalbatches' is not defined"
     ]
    }
   ],
   "source": [
    "#model = CNN_een\n",
    "\n",
    "#optimizer = Adam\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy()\n",
    "\n",
    "model = trainloop(\n",
    "            epochs=3,\n",
    "            model=model,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            learning_rate= 0.001,\n",
    "            loss_fn=loss_fn,\n",
    "            metrics=[accuracy],\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            log_dir=\"../../models/test/\",\n",
    "            train_steps=len(train_dataloader),\n",
    "            eval_steps=150,\n",
    "            #patience= ,\n",
    "            #factor= ,\n",
    "            #tunewriter= ,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.remove('../..')\n",
    "sys.path.remove('/home/admindme/code/ML22_opdracht1/notebooks')\n",
    "sys.path.insert(0, '/home/admindme/code/ML22_opdracht1/src')\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Changing the CWD\n",
    "os.chdir('../src')\n",
    "#os.chdir('../notebooks')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-mwI8DU6x-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5829ab1a7438afd0eb3e39a540a9bafca59bf334debab56f2c37d99237ff203a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
