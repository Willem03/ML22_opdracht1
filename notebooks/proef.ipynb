{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 17:55:26.969653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-17 17:55:28.051463: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-17 17:55:28.051567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-17 17:55:28.051577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.insert(0, \"../..\")\n",
    "\n",
    "from typing import Callable, Protocol, Dict, Optional, Iterator, List, Tuple\n",
    "\n",
    "import gin\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#from src.data import make_dataset\n",
    "#from src.models import metrics\n",
    "#from src.models import train_model # om de functie train_loop binnen te halen\n",
    "\n",
    "# ------------------------------------------------------------------------------- #\n",
    "## Alle imports benodigd voor de functie train_loop uit train_model.py \n",
    "\n",
    "import tensorflow as tf  # noqa: F401\n",
    "\n",
    "# needed to make summarywriter load without error\n",
    "from loguru import logger\n",
    "from numpy import Inf\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from src.models.metrics import Metric\n",
    "#from src.typehinting import GenericModel\n",
    "#from src.data import data_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## benodigde functie uitmake_dataset.py\n",
    "\n",
    "#@gin.configurable\n",
    "def get_MNIST(  # noqa: N802\n",
    "    data_dir: Path, batch_size: int\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=data_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benodigde funties uit metrics.py\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class Metric:\n",
    "    def __repr__(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Accuracy\"\n",
    "\n",
    "    def __call__(self, y: Tensor, yhat: Tensor) -> Tensor:\n",
    "        return (yhat.argmax(dim=1) == y).sum() / len(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericModel(Protocol):\n",
    "    train: Callable\n",
    "    eval: Callable\n",
    "    parameters: Callable\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> torch.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: GenericModel) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit data_tools.py\n",
    "\n",
    "def dir_add_timestamp(log_dir: Optional[Path] = None) -> Path:\n",
    "    if log_dir is None:\n",
    "        log_dir = Path(\".\")\n",
    "    log_dir = Path(log_dir)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    log_dir = log_dir / timestamp\n",
    "    logger.info(f\"Logging to {log_dir}\")\n",
    "    if not log_dir.exists():\n",
    "        log_dir.mkdir(parents=True)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definieren functie train_loop\n",
    "\n",
    "#@gin.configurable\n",
    "def trainloop(\n",
    "    epochs: int,\n",
    "    model: GenericModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    learning_rate: float,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    train_dataloader: Iterator,\n",
    "    test_dataloader: Iterator,\n",
    "    log_dir: Path,\n",
    "    train_steps: int,\n",
    "    eval_steps: int,\n",
    "    patience: int = 10,\n",
    "    factor: float = 0.9,\n",
    "    tunewriter: bool = False,\n",
    ") -> GenericModel:\n",
    "    \n",
    "    optimizer_: torch.optim.Optimizer = optimizer(\n",
    "        model.parameters(), lr=learning_rate\n",
    "    )  # type: ignore\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_,\n",
    "        factor=factor,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    if not tunewriter:\n",
    "        log_dir = dir_add_timestamp(log_dir)\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "        #write_gin(log_dir, gin.config_str())\n",
    "\n",
    "        images, _ = next(iter(train_dataloader))\n",
    "        if len(images.shape) == 4:\n",
    "            grid = make_grid(images)\n",
    "            writer.add_image(\"images\", grid)\n",
    "        writer.add_graph(model, images)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = trainbatches(\n",
    "            model, train_dataloader, loss_fn, optimizer_, train_steps\n",
    "        )\n",
    "\n",
    "        metric_dict, test_loss = evalbatches(\n",
    "            model, test_dataloader, loss_fn, metrics, eval_steps\n",
    "        )\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        if tunewriter:\n",
    "            tune.report(\n",
    "                iterations=epoch,\n",
    "                train_loss=train_loss,\n",
    "                test_loss=test_loss,\n",
    "                **metric_dict,\n",
    "            )\n",
    "        else:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "            for m in metric_dict:\n",
    "                writer.add_scalar(f\"metric/{m}\", metric_dict[m], epoch)\n",
    "            lr = [group[\"lr\"] for group in optimizer_.param_groups][0]\n",
    "            writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "            metric_scores = [f\"{v:.4f}\" for v in metric_dict.values()]\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} train {train_loss:.4f} test {test_loss:.4f} metric {metric_scores}\"  # noqa E501\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit train_model.py\n",
    "\n",
    "def trainbatches(\n",
    "    model: GenericModel,\n",
    "    traindatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_steps: int,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss: float = 0.0\n",
    "    for _ in tqdm(range(train_steps)):\n",
    "        x, y = next(iter(traindatastreamer))\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.detach().numpy()\n",
    "    train_loss /= train_steps\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uit train_model.py\n",
    "\n",
    "def evalbatches(\n",
    "    model: GenericModel,\n",
    "    testdatastreamer: Iterator,\n",
    "    loss_fn: Callable,\n",
    "    metrics: List[Metric],\n",
    "    eval_steps: int,\n",
    ") -> Tuple[Dict[str, float], float]:\n",
    "    model.eval()\n",
    "    test_loss: float = 0.0\n",
    "    metric_dict: Dict[str, float] = {}\n",
    "    for _ in range(eval_steps):\n",
    "        x, y = next(iter(testdatastreamer))\n",
    "        yhat = model(x)\n",
    "        test_loss += loss_fn(yhat, y).detach().numpy()\n",
    "        for m in metrics:\n",
    "            metric_dict[str(m)] = (\n",
    "                metric_dict.get(str(m), 0.0) + m(y, yhat).detach().numpy()\n",
    "            )\n",
    "\n",
    "    test_loss /= eval_steps\n",
    "    for key in metric_dict:\n",
    "        metric_dict[str(key)] = metric_dict[str(key)] / eval_steps\n",
    "    return metric_dict, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 1: CNN_een met een convolutional layer & drie linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_een_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6272, 3136),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3136, 1568),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1568, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_een_drie = CNN_een_drie(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.2: CNN_twee met twee convolutional layers & twee linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee_twee(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_twee_twee = CNN_twee_twee(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.3: CNN_twee met twee convolutional layers en drie linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_twee_drie = CNN_twee_drie(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2.4: CNN_twee met twee convolutional layers & vier linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_twee_vier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(576, 288),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(288, 144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(144, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_twee_vier = CNN_twee_vier(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.2: CNN_drie met twee convolutional layers & twee linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie_twee(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_drie_twee = CNN_drie_twee(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.3: CNN_drie met drie convolutional layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie_drie(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_drie_drie = CNN_drie_drie(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3.4: CNN_drie met drie convolutional layers & vier linear layers\n",
    "\n",
    "#@gin.configurable\n",
    "class CNN_drie_vier(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes: int, kernel_size: int, filter1: int, filter2: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(1, filter1, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filter1, filter2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convolutions(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n",
    "\n",
    "# a = aantal classes, b = kernel_size, c = grootte filter 1, d = grootte  filter2\n",
    "model_drie_vier = CNN_drie_vier(10, 3, 32, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellen = [\n",
    "            model_een_drie,\n",
    "            model_twee_twee,\n",
    "            model_twee_drie,\n",
    "            model_twee_vier,\n",
    "            model_drie_twee,\n",
    "            model_drie_drie,\n",
    "            model_drie_vier,\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in modellen:\n",
    "    print(summary(model, input_size=(64, 1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 157)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir =  Path(\"/home/admindme/code/ML22_opdracht1/data/raw/FashionMNIST\")\n",
    "train_dataloader, test_dataloader = get_MNIST(datadir, 64)\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 17:58:01.676 | INFO     | __main__:dir_add_timestamp:9 - Logging to ../../models/test/20221217-1758\n",
      "100%|██████████| 938/938 [00:26<00:00, 34.84it/s]\n",
      "2022-12-17 17:58:31.512 | INFO     | __main__:trainloop:68 - Epoch 0 train 0.7359 test 0.5302 metric ['0.7997']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.45it/s]\n",
      "2022-12-17 17:59:00.583 | INFO     | __main__:trainloop:68 - Epoch 1 train 0.4598 test 0.4386 metric ['0.8433']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.23it/s]\n",
      "2022-12-17 17:59:29.045 | INFO     | __main__:trainloop:68 - Epoch 2 train 0.3787 test 0.3804 metric ['0.8578']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.91it/s]\n",
      "2022-12-17 17:59:57.690 | INFO     | __main__:trainloop:68 - Epoch 3 train 0.3373 test 0.3247 metric ['0.8824']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.52it/s]\n",
      "2022-12-17 18:00:25.874 | INFO     | __main__:trainloop:68 - Epoch 4 train 0.3080 test 0.3263 metric ['0.8872']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.74it/s]\n",
      "2022-12-17 18:00:54.024 | INFO     | __main__:trainloop:68 - Epoch 5 train 0.2892 test 0.3295 metric ['0.8831']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.89it/s]\n",
      "2022-12-17 18:01:21.978 | INFO     | __main__:trainloop:68 - Epoch 6 train 0.2736 test 0.3023 metric ['0.8914']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.27it/s]\n",
      "2022-12-17 18:01:50.390 | INFO     | __main__:trainloop:68 - Epoch 7 train 0.2563 test 0.3039 metric ['0.8859']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.29it/s]\n",
      "2022-12-17 18:02:18.077 | INFO     | __main__:trainloop:68 - Epoch 8 train 0.2418 test 0.2949 metric ['0.8915']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.80it/s]\n",
      "2022-12-17 18:02:46.795 | INFO     | __main__:trainloop:68 - Epoch 9 train 0.2372 test 0.2652 metric ['0.9038']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.84it/s]\n",
      "2022-12-17 18:03:14.644 | INFO     | __main__:trainloop:68 - Epoch 10 train 0.2311 test 0.3146 metric ['0.8903']\n",
      "100%|██████████| 938/938 [00:24<00:00, 38.07it/s]\n",
      "2022-12-17 18:03:41.688 | INFO     | __main__:trainloop:68 - Epoch 11 train 0.2195 test 0.2794 metric ['0.9001']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.27it/s]\n",
      "2022-12-17 18:04:09.379 | INFO     | __main__:trainloop:68 - Epoch 12 train 0.2135 test 0.2851 metric ['0.9024']\n",
      "100%|██████████| 938/938 [00:24<00:00, 37.59it/s]\n",
      "2022-12-17 18:04:36.884 | INFO     | __main__:trainloop:68 - Epoch 13 train 0.2064 test 0.2728 metric ['0.8982']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.15it/s]\n",
      "2022-12-17 18:05:04.577 | INFO     | __main__:trainloop:68 - Epoch 14 train 0.1980 test 0.2736 metric ['0.9043']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.78it/s]\n",
      "2022-12-17 18:05:32.655 | INFO     | __main__:trainloop:68 - Epoch 15 train 0.1960 test 0.2718 metric ['0.9082']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.44it/s]\n",
      "2022-12-17 18:06:00.967 | INFO     | __main__:trainloop:68 - Epoch 16 train 0.1866 test 0.2970 metric ['0.8983']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.09it/s]\n",
      "2022-12-17 18:06:29.483 | INFO     | __main__:trainloop:68 - Epoch 17 train 0.1796 test 0.2811 metric ['0.9015']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.09it/s]\n",
      "2022-12-17 18:06:57.959 | INFO     | __main__:trainloop:68 - Epoch 18 train 0.1806 test 0.2723 metric ['0.9058']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.93it/s]\n",
      "2022-12-17 18:07:25.945 | INFO     | __main__:trainloop:68 - Epoch 19 train 0.1719 test 0.2560 metric ['0.9131']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.70it/s]\n",
      "2022-12-17 18:07:53.975 | INFO     | __main__:trainloop:68 - Epoch 20 train 0.1669 test 0.2845 metric ['0.8999']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.90it/s]\n",
      "2022-12-17 18:08:21.868 | INFO     | __main__:trainloop:68 - Epoch 21 train 0.1662 test 0.2700 metric ['0.9114']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.44it/s]\n",
      "2022-12-17 18:08:49.392 | INFO     | __main__:trainloop:68 - Epoch 22 train 0.1594 test 0.2646 metric ['0.9123']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.79it/s]\n",
      "2022-12-17 18:09:17.329 | INFO     | __main__:trainloop:68 - Epoch 23 train 0.1548 test 0.2810 metric ['0.9068']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.07it/s]\n",
      "2022-12-17 18:09:45.013 | INFO     | __main__:trainloop:68 - Epoch 24 train 0.1498 test 0.2744 metric ['0.9025']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.24it/s]\n",
      "2022-12-17 18:10:13.433 | INFO     | __main__:trainloop:68 - Epoch 25 train 0.1489 test 0.2801 metric ['0.9066']\n",
      "100%|██████████| 938/938 [00:24<00:00, 37.64it/s]\n",
      "2022-12-17 18:10:40.711 | INFO     | __main__:trainloop:68 - Epoch 26 train 0.1491 test 0.2898 metric ['0.9061']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.48it/s]\n",
      "2022-12-17 18:11:08.152 | INFO     | __main__:trainloop:68 - Epoch 27 train 0.1441 test 0.3009 metric ['0.9065']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.70it/s]\n",
      "2022-12-17 18:11:36.236 | INFO     | __main__:trainloop:68 - Epoch 28 train 0.1411 test 0.2669 metric ['0.9150']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.01it/s]\n",
      "2022-12-17 18:12:04.047 | INFO     | __main__:trainloop:68 - Epoch 29 train 0.1359 test 0.3009 metric ['0.9020']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.41it/s]\n",
      "2022-12-17 18:12:31.573 | INFO     | __main__:trainloop:68 - Epoch 30 train 0.1313 test 0.2794 metric ['0.9069']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.31it/s]\n",
      "2022-12-17 18:12:59.274 | INFO     | __main__:trainloop:68 - Epoch 31 train 0.1219 test 0.2832 metric ['0.9146']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.16it/s]\n",
      "2022-12-17 18:13:27.676 | INFO     | __main__:trainloop:68 - Epoch 32 train 0.1213 test 0.3074 metric ['0.9058']\n",
      "100%|██████████| 938/938 [00:24<00:00, 37.93it/s]\n",
      "2022-12-17 18:13:54.905 | INFO     | __main__:trainloop:68 - Epoch 33 train 0.1156 test 0.2835 metric ['0.9104']\n",
      "100%|██████████| 938/938 [00:24<00:00, 37.93it/s]\n",
      "2022-12-17 18:14:22.172 | INFO     | __main__:trainloop:68 - Epoch 34 train 0.1142 test 0.3217 metric ['0.9102']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.65it/s]\n",
      "2022-12-17 18:14:50.293 | INFO     | __main__:trainloop:68 - Epoch 35 train 0.1160 test 0.3152 metric ['0.9108']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.39it/s]\n",
      "2022-12-17 18:15:17.691 | INFO     | __main__:trainloop:68 - Epoch 36 train 0.1111 test 0.3318 metric ['0.9115']\n",
      "100%|██████████| 938/938 [00:24<00:00, 37.52it/s]\n",
      "2022-12-17 18:15:45.274 | INFO     | __main__:trainloop:68 - Epoch 37 train 0.1086 test 0.3231 metric ['0.9078']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.29it/s]\n",
      "2022-12-17 18:16:14.431 | INFO     | __main__:trainloop:68 - Epoch 38 train 0.1052 test 0.3507 metric ['0.9027']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.67it/s]\n",
      "2022-12-17 18:16:43.318 | INFO     | __main__:trainloop:68 - Epoch 39 train 0.1063 test 0.3263 metric ['0.9066']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.33it/s]\n",
      "2022-12-17 18:17:12.447 | INFO     | __main__:trainloop:68 - Epoch 40 train 0.1015 test 0.3135 metric ['0.9108']\n",
      "100%|██████████| 938/938 [00:25<00:00, 36.61it/s]\n",
      "2022-12-17 18:17:40.551 | INFO     | __main__:trainloop:68 - Epoch 41 train 0.1011 test 0.3487 metric ['0.9093']\n",
      "100%|██████████| 938/938 [00:25<00:00, 37.35it/s]\n",
      "2022-12-17 18:18:08.217 | INFO     | __main__:trainloop:68 - Epoch 42 train 0.0954 test 0.3784 metric ['0.9087']\n",
      "100%|██████████| 938/938 [00:27<00:00, 34.23it/s]\n",
      "2022-12-17 18:18:38.202 | INFO     | __main__:trainloop:68 - Epoch 43 train 0.0942 test 0.3506 metric ['0.9109']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.87it/s]\n",
      "2022-12-17 18:19:07.135 | INFO     | __main__:trainloop:68 - Epoch 44 train 0.0883 test 0.3778 metric ['0.9066']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.52it/s]\n",
      "2022-12-17 18:19:36.104 | INFO     | __main__:trainloop:68 - Epoch 45 train 0.0864 test 0.3726 metric ['0.9052']\n",
      "100%|██████████| 938/938 [00:26<00:00, 35.50it/s]\n",
      "2022-12-17 18:20:05.043 | INFO     | __main__:trainloop:68 - Epoch 46 train 0.0888 test 0.3681 metric ['0.9056']\n",
      " 71%|███████   | 663/938 [00:18<00:07, 35.23it/s]\n",
      " 47%|████▋     | 47/100 [22:21<25:13, 28.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      9\u001b[0m accuracy \u001b[39m=\u001b[39m Accuracy()\n\u001b[0;32m---> 11\u001b[0m model \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m     12\u001b[0m             epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     13\u001b[0m             model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     14\u001b[0m             optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m     15\u001b[0m             learning_rate\u001b[39m=\u001b[39;49m \u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m             loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     17\u001b[0m             metrics\u001b[39m=\u001b[39;49m[accuracy],\n\u001b[1;32m     18\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     19\u001b[0m             test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m     20\u001b[0m             log_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../../models/test/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m             train_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_dataloader),\n\u001b[1;32m     22\u001b[0m             eval_steps\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m             \u001b[39m#patience= ,\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m             \u001b[39m#factor= ,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m             \u001b[39m#tunewriter= ,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m             )\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m     40\u001b[0m     writer\u001b[39m.\u001b[39madd_graph(model, images)\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 43\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m     44\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     47\u001b[0m     metric_dict, test_loss \u001b[39m=\u001b[39m evalbatches(\n\u001b[1;32m     48\u001b[0m         model, test_dataloader, loss_fn, metrics, eval_steps\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(test_loss)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrainbatches\u001b[0;34m(model, traindatastreamer, loss_fn, optimizer, train_steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m train_loss: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(train_steps)):\n\u001b[0;32m---> 13\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(traindatastreamer))\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m     yhat \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deep-learning-mwI8DU6x-py3.9/lib/python3.9/site-packages/torchvision/transforms/functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[0;32m--> 167\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[1;32m    168\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_drie_drie\n",
    "epochs = 100\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy()\n",
    "\n",
    "model = trainloop(\n",
    "            epochs=epochs,\n",
    "            model=model,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            learning_rate= 0.001,\n",
    "            loss_fn=loss_fn,\n",
    "            metrics=[accuracy],\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            log_dir=\"../../models/test/\",\n",
    "            train_steps=len(train_dataloader),\n",
    "            eval_steps=150,\n",
    "            #patience= ,\n",
    "            #factor= ,\n",
    "            #tunewriter= ,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-mwI8DU6x-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5829ab1a7438afd0eb3e39a540a9bafca59bf334debab56f2c37d99237ff203a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
